Here‚Äôs a full side-by-side response for **Q3 and Q4 (QX & QY)** across all years (2022‚Äì2024) from the DSBDA exam paper, based on your three prompts:

---

### ‚úÖ **Prompt 1: Group all QX and QY (Q3 & Q4) questions topic-wise with exact questions (include repeated ones)**

| **Topic**                            | **Exact Questions from Q3 & Q4 (2022‚Äì2024)**                                                                                                                                                                                                                                                                                                                      |
| ------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Types of Analytics**               | - What are the types of analytics in big data? Explain in brief. (2023 Jan)<br> - List and explain the various types of analytics in Big data. (2023 Dec)                                                                                                                                                                                                         |
| **Association Rule Mining**          | - What are association rules? Explain Apriori Algorithm in brief. (2022)<br> - How Apriori Algorithm works, explain with suitable example? (2023 June)<br> - How does the Apriori algorithm discover frequent itemsets in a dataset? What is the role of support and confidence in the context of association rule mining using the Apriori algorithm? (2024 May) |
| **Support & Confidence Calculation** | - Calculate the support and confidence value for all the possible item sets. (2023 Jan)<br> - Calculates the support and confidence value for all the possible item sets. (2023 Dec)                                                                                                                                                                              |
| **Decision Trees**                   | - Explain why decision trees are used. Draw a sample decision tree and explain its parts. (2023 June)<br> - Explain the process of building a decision tree? What are the criteria used for splitting nodes in a decision tree? (2024 May)                                                                                                                        |
| **Regression (Linear, Logistic)**    | - Explain Linear Regression and Logistic Regression. (2022)<br> - Explain the use of logistic function in logistic regression in detail. (2023 Jan)<br> - What is logistic regression, and how does it differ from linear regression? What is the sigmoid function, and what role does it play in logistic regression? (2024 May)                                 |
| **Data Preprocessing**               | - Explain various data pre-processing steps. Discuss essential python libraries. (2022)<br> - Removing duplicates, missing data, transformation. (2023 Jan & Dec)<br> - What is data preprocessing? Explain handling missing data and transformation. (2023 June)                                                                                                 |
| **Naive Bayes Classifier**           | - Explain Na√Øve Bayes‚Äô classifier and its applications. (2023 June)<br> - Given features "offer" and "free", classify spam using Naive Bayes. (2024 May)                                                                                                                                                                                                          |

---

### ‚úÖ **Prompt 2: Frequency summary of grouped topics from QX & QY (2022‚Äì2024)**

| **Topic**                    | **Frequency** |
| ---------------------------- | ------------- |
| Association Rule Mining      | 3             |
| Data Preprocessing           | 3             |
| Types of Analytics           | 2             |
| Support & Confidence Calc.   | 2             |
| Regression (Linear/Logistic) | 3             |
| Decision Trees               | 2             |
| Naive Bayes Classifier       | 2             |

---

### ‚úÖ **Prompt 3: Detailed Year-wise Topic Grouping (Q3 & Q4)**

---

#### üóìÔ∏è **2022**

* **Group 1: Data Preprocessing + Association Rules**

  * Q3 a) Explain various data pre-processing steps. Discuss essential python libraries for preprocessing.
  * Q3 b) What are association rules? Explain Apriori Algorithm in brief.

* **Group 2: Regression**

  * Q4 a) Explain the following:<br> i) Linear Regression<br> ii) Logistic Regression
  * Q4 b) Explain scikit-learn library for matplotlib with example.

---

#### üóìÔ∏è **2023 Jan**

* **Group 1: Types of Analytics + Support & Confidence**

  * Q3 a) What are the types of analytics in big data?
  * Q3 b) Calculate the support and confidence value for all the possible item sets.

* **Group 2: Regression + Preprocessing**

  * Q4 a) Explain the use of logistic function in logistic regression in detail.
  * Q4 b) Write short note on:<br> i) Removing duplicates<br> ii) Handling missing data<br> iii) Data transformation.

---

#### üóìÔ∏è **2023 June**

* **Group 1: Decision Tree + Association Rule Mining**

  * Q3 a) Explain why decision trees are used. Draw a sample decision tree and explain its parts.
  * Q3 b) How Apriori Algorithm works, explain with suitable example?

* **Group 2: Data Preprocessing + Naive Bayes**

  * Q4 a) What is data preprocessing? Explain in details about handling missing data and transformation.
  * Q4 b) Explain Na√Øve Bayes‚Äô classifier and its applications.

---

#### üóìÔ∏è **2023 Dec**

* **Group 1: Types of Analytics + Support & Confidence**

  * Q3 a) List and explain the various types of analytics in Big data.
  * Q3 b) Calculates the support and confidence value for all the possible item sets.

* **Group 2: Data Preprocessing (repeat)**

  * Q4 a) Removing Duplicates from dataset.<br> ii) Handling Missing Data

---

#### üóìÔ∏è **2024 May**

* **Group 1: Logistic Regression + Naive Bayes**

  * Q3 a) What is logistic regression, and how does it differ from linear regression? What is the sigmoid function, and what role does it play?
  * Q3 b) Use Naive Bayes to classify email with features "offer"=1 and "free"=1.

* **Group 2: Apriori Algorithm + Decision Tree**

  * Q4 a) How does Apriori algorithm discover frequent itemsets?
  * Q4 b) Explain building a decision tree and criteria used for splitting nodes.

---

Let me know if you want this extended for Q5 & Q6 (or Q7 & Q8), or want predicted questions and answers based on this analysis.



Here is the complete topic-wise answer key for Q3 & Q4 (2022‚Äì2024) from DSBDA, with detailed questions, sub-question answers (with diagrams), year+group reference, and 9+ points per answer as per your format:

---

### üìòTopic: **Types of Analytics**

**Question (2√ó)**

* What are the types of analytics in Big Data? Explain in brief. (2023 Jan, 2023 Dec) ‚Äî **\[Freq: 2]**

**Answer:**
Types of analytics in big data:

1. **Descriptive** ‚Äì What happened? (e.g., sales reports)
2. **Diagnostic** ‚Äì Why did it happen? (e.g., root cause analysis)
3. **Predictive** ‚Äì What is likely to happen? (e.g., ML for forecasting)
4. **Prescriptive** ‚Äì What should be done? (e.g., decision systems)
5. **Cognitive** ‚Äì AI-based decision enhancement
6. Helps transform raw data into actionable insights
7. Used across marketing, healthcare, finance, etc.
8. Based on past, current, or predictive inputs
9. Enhances business decisions and strategies

**Diagram:**

```
[Descriptive] ‚Üí [Diagnostic] ‚Üí [Predictive] ‚Üí [Prescriptive]
```

**Year & Group:** 2023-Jan (Q3a), 2023-Dec (Q3a)

---

### üìòTopic: **Association Rule Mining (Apriori Algorithm)**

**Question (3√ó)**

* What are association rules? Explain Apriori Algorithm in brief. (2022)
* How Apriori Algorithm works, explain with suitable example? (2023 June)
* How does Apriori algorithm discover frequent itemsets... explain role of support & confidence? (2024 May) ‚Äî **\[Freq: 3]**

**Answer:**

1. **Association Rule:** A ‚Üí B implies A and B occur together
2. **Support:** freq(A‚à©B)/total transactions
3. **Confidence:** freq(A‚à©B)/freq(A)
4. **Apriori Steps:**

   * Generate 1-itemsets
   * Prune below minsup
   * Combine to form k-itemsets
   * Repeat until no new sets
5. Used for market basket analysis
6. Improves product recommendations
7. Efficient using support thresholds
8. Can generate hundreds of rules
9. Helps understand customer behavior

**Example:**
T1: Milk, Bread
T2: Milk, Diaper, Beer
‚Üí Rule: {Milk} ‚áí {Bread}

* Support: 1/2
* Confidence: 1/1

**Diagram:**

```
[Transactions] ‚Üí [Frequent Itemsets] ‚Üí [Association Rules]
```

**Year & Group:** 2022 (Q3b), 2023-June (Q3b), 2024-May (Q4a)

---

### üìòTopic: **Support & Confidence Calculation**

**Question (2√ó)**

* Calculate the support and confidence value for all possible item sets. (2023 Jan, 2023 Dec) ‚Äî **\[Freq: 2]**

**Answer:**
Given transactions:
1: Onion, Potato, Cold Drink
2: Onion, Burger, Cold Drink
3: Eggs, Onion, Cold Drink
4: Potato, Milk, Eggs
5: Potato, Burger, Cold Drink, Milk, Eggs

1. Count all itemsets
2. **Support** = freq(itemset) / total transactions
3. **Confidence** = freq(X‚à™Y) / freq(X)
4. e.g., Support({Onion, Cold Drink}) = 3/5
5. Confidence({Onion} ‚áí {Cold Drink}) = 3/3
6. Compute for all combinations
7. Prune rules below min confidence
8. Shows strong purchase links
9. Applied in recommendation engines

**Diagram:**

```
[Itemsets] ‚Üí [Support] ‚Üí [Confidence] ‚Üí [Prune Rules]
```

**Year & Group:** 2023-Jan (Q3b), 2023-Dec (Q3b)

---

### üìòTopic: **Decision Trees**

**Question (2√ó)**

* Explain why decision trees are used. Draw a sample decision tree and explain its parts. (2023 June)
* Explain building a decision tree, and criteria used for splitting. (2024 May) ‚Äî **\[Freq: 2]**

**Answer:**

1. Decision Trees = classification models
2. Easy to visualize and interpret
3. Nodes = decision conditions
4. Leaves = class labels
5. Built using top-down greedy algorithms
6. Use metrics like Gini Index, Entropy, Info Gain
7. Split recursively until pure or max depth
8. Handle both numerical and categorical data
9. Used in customer churn, fraud detection

**Diagram:**

```
         [Income > 50K?]
            /     \
          Yes     No
         /         \
    [Age < 30?]    Reject
     /    \
  Yes     No
Buy     Reject
```

**Year & Group:** 2023-June (Q3a), 2024-May (Q4b)

---

### üìòTopic: **Regression (Linear + Logistic)**

**Question (3√ó)**

* Explain Linear Regression and Logistic Regression. (2022)
* Use of logistic function in logistic regression. (2023 Jan)
* What is logistic regression... sigmoid function... (2024 May) ‚Äî **\[Freq: 3]**

**Answer:**
üü© **Linear Regression**:

1. Predicts continuous outcome
2. Uses best-fit line: Y = aX + b
3. Minimize error (least squares)
4. Assumes linearity
5. Useful in trend analysis

üü© **Logistic Regression**:

1. Predicts binary outcome (yes/no)
2. Uses sigmoid function: 1/(1+e^-z)
3. Output in 0‚Äì1 range (probability)
4. Uses log loss
5. Common in spam detection

üü© **Difference**:

* Linear = regression, Logistic = classification
* Output type differs
* Loss function differs

**Diagram (Logistic):**

```
         |
       1 |        ~~~~
         |     ~~
     0.5 |~~~
         |       
       0 +-----------------
            z (wx + b)
```

**Year & Group:** 2022 (Q4a), 2023-Jan (Q4a), 2024-May (Q3a)

---

### üìòTopic: **Data Preprocessing**

**Question (3√ó)**

* Explain preprocessing steps. (2022)<br> - Remove duplicates, handle missing data, transform. (2023 Jan & Dec)<br> - Explain handling missing data and transformation. (2023 June) ‚Äî **\[Freq: 3]**

**Answer:**

1. **Missing Values** ‚Äì Fill with mean/median or drop
2. **Duplicates** ‚Äì Remove using `.drop_duplicates()`
3. **Normalization** ‚Äì Scale values to 0‚Äì1
4. **Encoding** ‚Äì Convert text to numeric
5. **Outlier Removal** ‚Äì IQR or Z-score method
6. **Transformation** ‚Äì Log/sqrt for skewed data
7. **Feature Extraction** ‚Äì Combine fields
8. **Libraries** ‚Äì pandas, numpy, sklearn
9. **Data Cleaning** ‚Äì Improves model performance

**Diagram:**

```
[Raw Data] ‚Üí [Clean] ‚Üí [Transform] ‚Üí [Model Input]
```

**Year & Group:** 2022 (Q3a), 2023-Jan (Q4b), 2023-Dec (Q4a), 2023-June (Q4a)

---

### üìòTopic: **Naive Bayes Classifier**

**Question (2√ó)**

* Explain Naive Bayes‚Äô classifier and its applications. (2023 June)
* Classify spam using Naive Bayes (features: offer=1, free=1). (2024 May) ‚Äî **\[Freq: 2]**

**Answer:**

1. Probabilistic classifier
2. Based on Bayes‚Äô theorem
3. Assumes features are independent
4. Calculates P(Class|Features) ‚àù P(Features|Class) √ó P(Class)
5. Common in spam filtering
6. Fast and scalable
7. Needs small training data
8. Works well with text data
9. Example: classify email using P(spam|offer, free)

**Diagram:**

```
[Features] ‚Üí [P(spam|features)] ‚Üí [Highest Prob] ‚Üí [Class Output]
```

**Year & Group:** 2023-June (Q4b), 2024-May (Q3b)

---

Let me know if you want the same for Q5 & Q6 or in PDF format for revision.






Here‚Äôs your **Smart Preparation Strategy** for Q3 & Q4 (QX & QY) from DSBDA:

---

### ‚úÖ **Prompt 4: 2 Topic Groups to Safely Prepare for QX or QY**

Based on 2022‚Äì2024 paper trends, you should prepare these **two safest topic groups** (high frequency + broad coverage):

#### üîí **Group 1: Association Rule Mining + Support & Confidence + Decision Trees**

* Appeared in **every year**
* Covers Apriori + rule metrics + classification using trees
* Scoring questions with frequent diagrams

#### üîí **Group 2: Regression (Logistic + Linear) + Data Preprocessing**

* Also appeared in **every year**
* Combines theory + math-based logic + applied data cleaning

---

### ‚úÖ **Prompt 5: Prediction for This Year‚Äôs Q3 & Q4 (2025)**

---

#### üîÆ **Predicted QX (Q3)**

* **Topic:** Association Rule Mining + Decision Trees
* **Question:**
  **a)** How does the Apriori algorithm discover frequent itemsets? Explain with an example.
  **b)** Explain how decision trees are built. Describe criteria for node splitting and draw a simple tree.

**Answer a (Apriori Algorithm):**

1. Step 1: Generate 1-itemsets
2. Step 2: Prune using min support
3. Step 3: Join to form k-itemsets
4. Step 4: Repeat until no new itemsets
5. Use support = freq(X‚à™Y)/total
6. Use confidence = freq(X‚à™Y)/freq(X)
7. Frequent itemsets give strong rules
8. Example: {Milk, Bread}
9. Used in market basket analysis

**Diagram:**

```
[Transactions]
     ‚Üì
[Frequent 1-Itemsets]
     ‚Üì
[Candidate k-Itemsets]
     ‚Üì
[Rules: A ‚áí B (Support, Confidence)]
```

**Answer b (Decision Trees):**

1. Root node splits based on highest info gain
2. Gini or entropy used for split evaluation
3. Internal nodes = decision points
4. Leaves = final class labels
5. Recursive partitioning
6. Stops when pure or max depth reached
7. Easy to interpret
8. Can overfit (need pruning)
9. Used for classification problems

**Diagram:**

```
        [Weather?]
         /     \
     Sunny    Rainy
     /           \
  [Humidity?]     No
   /    \
Yes     No
```

---

#### üîÆ **Predicted QY (Q4)**

* **Topic:** Logistic Regression + Data Preprocessing
* **Question:**
  **a)** What is logistic regression? Explain sigmoid function with an example.
  **b)** Explain how missing values are handled and how data is transformed in preprocessing.

**Answer a (Logistic Regression):**

1. Used for binary classification
2. Output = probability (0‚Äì1)
3. Uses sigmoid: 1 / (1 + e^-z)
4. z = weighted sum of inputs
5. Predicts class based on threshold
6. Optimized using log loss
7. Example: spam detection
8. Feature scaling helps performance
9. Decision boundary: linear in feature space

**Diagram:**

```
     |
   1 |      ~~~~
     |    ~
 0.5 |~~
     |
   0 +----------------
        z (wx + b)
```

**Answer b (Preprocessing):**

1. Missing values: drop or fill (mean, median)
2. Outliers: removed via IQR or Z-score
3. Encoding: categorical ‚Üí numeric
4. Normalization: min-max or z-score
5. Transformation: log, sqrt for skewed data
6. Feature scaling: brings uniformity
7. Remove duplicates
8. Library tools: pandas, sklearn
9. Essential before modeling

**Diagram:**

```
[Raw Data]
   ‚Üì
[Clean ‚Üí Encode ‚Üí Scale]
   ‚Üì
[Model Input Ready]
```

---

Would you like this prediction set as a printable PDF or flashcard? Let me know!





Here is your **Keyword Strategy** for scoring high in any Q3 & Q4 (QX & QY) DSBDA answer, including **topic-specific keywords** and **universal keywords** that boost marks.

---

### ‚úÖ **Prompt 6: 9 Keywords per QX & QY Topic (2022‚Äì2024)**

| **Topic**                   | **Keywords (9)**                                                                                      |
| --------------------------- | ----------------------------------------------------------------------------------------------------- |
| **Association Rule Mining** | Apriori, frequent itemset, support, confidence, transaction, pruning, minsup, rule, pattern           |
| **Support & Confidence**    | support, confidence, itemset, lift, count, probability, frequent, threshold, dataset                  |
| **Decision Trees**          | node, leaf, entropy, Gini index, info gain, splitting, pruning, classification, depth                 |
| **Logistic Regression**     | classification, sigmoid, probability, threshold, log loss, weights, binary, output, logistic function |
| **Linear Regression**       | Y=aX+b, prediction, line of best fit, continuous, error, slope, intercept, R¬≤, MSE                    |
| **Data Preprocessing**      | missing, duplicate, encoding, normalization, transformation, cleaning, outlier, scaling, pipeline     |
| **Types of Analytics**      | descriptive, diagnostic, predictive, prescriptive, insight, pattern, action, decision, trend          |
| **Naive Bayes Classifier**  | probability, independence, Bayes theorem, prior, likelihood, posterior, spam, class, conditional      |

---

### ‚úÖ **Prompt 7: 10 Universal Keywords (Safe for Any Q3/Q4 Answer)**

| **Keyword**       | **Why It Works in Any Answer**                                                               |
| ----------------- | -------------------------------------------------------------------------------------------- |
| **data**          | Core of any analytics task ‚Äì makes your answer sound grounded                                |
| **model**         | Used in classification, prediction, and regression ‚Äì shows you understand processes          |
| **accuracy**      | Evaluation metric for nearly every model ‚Äì earns marks even if rough                         |
| **prediction**    | Central to regression, classification, association rules ‚Äì always relevant                   |
| **visualization** | Even if not asked, suggesting it improves communication of results ‚Äì bonus mark booster      |
| **tools**         | Mentioning libraries like `scikit-learn`, `pandas`, `R` makes your answer technically rich   |
| **scalability**   | Smart addition to show your understanding of real-world big data needs                       |
| **pipeline**      | Used in preprocessing and modeling ‚Äì shows awareness of workflow                             |
| **stakeholders**  | Adds depth in answers involving analytics projects or lifecycle                              |
| **automation**    | Works in data cleaning, transformation, deployment ‚Äì shows relevance in real-world use cases |

---

üß† **Tip:** Use **3‚Äì5 topic keywords** + **2‚Äì3 universal keywords** + a **diagram** to get strong partial marks even for incomplete answers.

Would you like these turned into a **flashcard deck** or **quick cheat sheet PDF**?


Here are **9 smart universal points** you can safely write in **any Q3 or Q4 answer** (from DSBDA) to boost marks‚Äîeven if you‚Äôre unsure of the full answer.

Each point includes **at least one universal keyword** to help you score partial marks confidently:

---

### ‚úÖ **Prompt 8: 9 High-Scoring Universal Fillers**

1. **"Most models aim to improve prediction accuracy using relevant data features."**
   ‚Üí Works for regression, classification, decision trees, etc.

2. **"Data preprocessing is essential before model building to ensure clean and structured inputs."**
   ‚Üí Safe line for any question involving modeling or data.

3. **"Tools like Python, scikit-learn, and R are widely used in analytics pipelines."**
   ‚Üí Adds a technical layer to any answer.

4. **"Stakeholders rely on visualization to interpret outcomes and take action."**
   ‚Üí Great filler in decision tree, types of analytics, or even regression questions.

5. **"Accuracy, precision, and recall are key metrics used to evaluate model performance."**
   ‚Üí Always relevant, especially in classification/regression answers.

6. **"Scalability is crucial when dealing with large volumes of data in real-world scenarios."**
   ‚Üí Shows big-data awareness; fits in preprocessing or modeling context.

7. **"Automation in data cleaning and transformation boosts the efficiency of analytics workflows."**
   ‚Üí Works in preprocessing, tools, or lifecycle questions.

8. **"Any model selection should consider both data characteristics and business objectives."**
   ‚Üí Use in regression/classification or analytics lifecycle.

9. **"A well-structured pipeline improves consistency across different phases of analytics projects."**
   ‚Üí Great as a closer in any model/preprocessing answer.

---

üß† **Pro Tip:** Use 2‚Äì3 of these universal points + 5 topic-specific points + a simple diagram = safe partial/full marks even under pressure.

Let me know if you want this as a **revision sticker** or **mini cheat sheet**.


### üìò DSBDA Q3 & Q4 Revision Notes (2022‚Äì2024)

---

#### üìò Topic 1: Association Rule Mining (Apriori)

**Answer:**
Apriori algorithm discovers frequent itemsets using support and confidence. Used in market basket analysis.

**Keywords:** Apriori, itemset, support, confidence, pattern, rules, transaction, prune, recommendation

**Diagram:**

```
[Transactions] ‚Üí [1-Itemsets] ‚Üí [k-Itemsets] ‚Üí [Rules: A ‚Üí B]
```

---

#### üìò Topic 2: Support & Confidence Calculation

**Answer:**
Support = freq(X‚à™Y)/total, Confidence = freq(X‚à™Y)/freq(X). Used to evaluate strength of association rules.

**Keywords:** support, confidence, probability, itemset, dataset, rule strength, pattern, frequency, pruning

**Diagram:**

```
[Itemsets] ‚Üí [Support] ‚Üí [Confidence] ‚Üí [Strong Rules]
```

---

#### üìò Topic 3: Decision Trees

**Answer:**
A classification algorithm that splits data into branches based on criteria like Gini or Entropy.

**Keywords:** node, leaf, split, Gini, entropy, gain, prune, classification, depth

**Diagram:**

```
        [Feature1?]
         /     \
     Yes       No
    /           \
[Leaf1]       [Feature2?]
                 /   \
             Yes     No
           [Leaf2] [Leaf3]
```

---

#### üìò Topic 4: Logistic Regression

**Answer:**
Used for binary classification. Uses sigmoid function to output probabilities between 0‚Äì1.

**Keywords:** logistic, sigmoid, classification, probability, threshold, log loss, binary, regression, curve

**Diagram:**

```
     |
   1 |     ~~~~
     |   ~
 0.5 |~
     |
   0 +-------------
        z (wx + b)
```

---

#### üìò Topic 5: Linear Regression

**Answer:**
Used to predict continuous values based on linear relationships between variables.

**Keywords:** prediction, continuous, best-fit, error, slope, intercept, Y=WX+b, MSE, regression line

**Diagram:**

```
   Y
   |       *
   |     *
   |   *      *
   | *___________ X
       Line of Best Fit
```

---

#### üìò Topic 6: Data Preprocessing

**Answer:**
Prepares raw data for modeling. Includes cleaning, handling missing values, normalization, encoding, etc.

**Keywords:** cleaning, missing, duplicate, encoding, normalization, transformation, scaling, pipeline, outlier

**Diagram:**

```
[Raw Data] ‚Üí [Clean ‚Üí Encode ‚Üí Normalize ‚Üí Transform] ‚Üí [Model Input]
```

---

#### üìò Topic 7: Types of Analytics

**Answer:**
Analytics types: Descriptive (what happened), Diagnostic (why), Predictive (what may happen), Prescriptive (what to do).

**Keywords:** descriptive, diagnostic, predictive, prescriptive, insight, data, trend, decision, action

**Diagram:**

```
[Raw Data] ‚Üí [Descriptive] ‚Üí [Diagnostic] ‚Üí [Predictive] ‚Üí [Prescriptive]
```

---

#### üìò Topic 8: Naive Bayes Classifier

**Answer:**
Probabilistic classifier using Bayes‚Äô theorem. Assumes feature independence. Used in spam filtering, etc.

**Keywords:** probability, prior, likelihood, posterior, conditional, spam, features, naive, classifier

**Diagram:**

```
[P(Class|X)] ‚àù [P(X|Class) * P(Class)]
           ‚Üì
      Highest = Predicted Class
```

---

